{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#raw-denoising-project","title":"Raw-Denoising-Project","text":"<p>Training of SCUNet for 4-channel RAW image denoising and comparison to DnCNN.</p> <p>The Project went from May to August 2024.</p>"},{"location":"#this-websites-content","title":"This Website's Content","text":"<ul> <li>How to replicate this project</li> <li>Use of virtual environments (venv)</li> <li>Everything Python</li> <li>Everything MATLAB</li> <li>General tips for taining of neural networks</li> <li>List of trained models</li> <li>Showcase of results</li> </ul>"},{"location":"02_setup/","title":"Setup","text":"<p>Here's a guide for installing the project necessities, with separate tutorials for Windows and macOS.</p>"},{"location":"02_setup/#python-installation","title":"Python Installation","text":""},{"location":"02_setup/#windows","title":"Windows","text":""},{"location":"02_setup/#1-install-git","title":"1. Install Git","text":"<p>Choose one of the following methods:</p> <ul> <li> <p>Using winget tool in PowerShell or Command Prompt (requires terminal restart afterwards to take effect):   <pre><code>winget install --id Git.Git -e --source winget\n</code></pre></p> </li> <li> <p>Or download the standalone installer from https://git-scm.com/download/win</p> </li> </ul>"},{"location":"02_setup/#2-navigate-to-project-folder","title":"2. Navigate to Project Folder","text":"<pre><code>cd &lt;project folder&gt;\n</code></pre>"},{"location":"02_setup/#3-check-installed-python-versions","title":"3. Check Installed Python Versions","text":"<p>To see which Python versions are installed on your system, run: <pre><code>py -0p\n</code></pre></p>"},{"location":"02_setup/#4-create-a-virtual-environment","title":"4. Create a Virtual Environment","text":"<ul> <li>If Python 3.11 is installed along with other versions:   <pre><code>py -3.11 -m venv .venv\n</code></pre></li> <li>If only Python 3.11 is installed:   <pre><code>py -m venv .venv\n</code></pre></li> </ul>"},{"location":"02_setup/#5-resolve-execution-policy-error","title":"5. Resolve Execution Policy Error","text":"<p>If you encounter an \"about_Execution_Policies\" error, run this command and retry step 4.: <pre><code>Set-ExecutionPolicy Unrestricted -Scope CurrentUser\n</code></pre></p>"},{"location":"02_setup/#6-activate-the-virtual-environment","title":"6. Activate the Virtual Environment","text":"<pre><code>.venv\\Scripts\\activate\n</code></pre>"},{"location":"02_setup/#7-install-requirements","title":"7. Install Requirements","text":"<pre><code>pip install -r requirements_win.txt\n</code></pre>"},{"location":"02_setup/#macos","title":"macOS","text":""},{"location":"02_setup/#1-install-git_1","title":"1. Install Git","text":"<p>Choose one of the following methods:</p> <ul> <li> <p>Using Homebrew:   <pre><code>brew install git\n</code></pre></p> </li> <li> <p>Or choose another installation method from https://git-scm.com/download/mac</p> </li> </ul>"},{"location":"02_setup/#2-navigate-to-project-folder_1","title":"2. Navigate to Project Folder","text":"<pre><code>cd &lt;project folder&gt;\n</code></pre>"},{"location":"02_setup/#3-create-a-virtual-environment","title":"3. Create a Virtual Environment","text":"<ul> <li>For Python 3.11 (replace with your desired version if different):   <pre><code>python3.11 -m venv .venv\n</code></pre></li> <li>If you want to use the default Python 3 version:   <pre><code>python3 -m venv .venv\n</code></pre></li> </ul>"},{"location":"02_setup/#4-activate-the-virtual-environment","title":"4. Activate the Virtual Environment","text":"<pre><code>source .venv/bin/activate\n</code></pre>"},{"location":"02_setup/#5-install-requirements","title":"5. Install Requirements","text":"<pre><code>pip install -r requirements_mac.txt\n</code></pre>"},{"location":"02_setup/#matlab-installation","title":"MATLAB Installation","text":"<p>This is how to set up Python for MATLAB.</p>"},{"location":"02_setup/#1-download-python-3119","title":"1. Download Python 3.11.9","text":"<ul> <li>Download from the official site (DO NOT download from MS Store):</li> <li>Python 3.11.9 for Windows (amd64)</li> <li>During installation, ensure you check the box for \"Add python.exe to PATH\".</li> </ul>"},{"location":"02_setup/#2-navigate-to-the-matlab-project-folder-via-windows-command-prompt","title":"2. Navigate to the MATLAB Project Folder via Windows Command Prompt","text":"<pre><code>cd &lt;project folder&gt;\n</code></pre>"},{"location":"02_setup/#3-check-installed-python-versions_1","title":"3. Check Installed Python Versions","text":"<pre><code>py -0p\n</code></pre>"},{"location":"02_setup/#4-create-a-virtual-environment_1","title":"4. Create a Virtual Environment","text":"<ul> <li>Make a virtual environment to store Python modules by typing:   <pre><code>py -3.11 -m venv .venv\n</code></pre></li> <li>If multiple Python versions are installed, use:   <pre><code>py -3.11 -m venv .venv\n</code></pre></li> <li>If only Python 3.11 is installed, use:   <pre><code>py -m venv .venv\n</code></pre></li> </ul>"},{"location":"02_setup/#5-activate-the-virtual-environment","title":"5. Activate the Virtual Environment","text":"<ul> <li>Activate the virtual environment via:   <pre><code>.venv\\Scripts\\activate\n</code></pre></li> </ul>"},{"location":"02_setup/#6-install-required-packages","title":"6. Install Required Packages","text":"<ul> <li>Install the necessary packages with CUDA support into the new virtual environment:   <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121  \npip install numpy einops timm torchsummary\n</code></pre></li> </ul>"},{"location":"02_setup/#7-set-the-python-environment-in-matlab","title":"7. Set the Python Environment in MATLAB","text":"<ul> <li>In MATLAB console, specify the Python environment:   <pre><code>pyenv('Version', 'project_folder\\.venv\\Scripts\\pythonw.exe')\n</code></pre></li> <li>The result should look like this:   ```matlab   ans = </li> </ul> <p>PythonEnvironment with properties:</p> <pre><code>      Version: \"3.11\"\n   Executable: \"project_folder\\.venv\\Scripts\\pythonw.exe\"\n      Library: \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\python311.dll\"\n         Home: \"project_folder\\.venv\\Scripts\\.venv\"\n       Status: NotLoaded\nExecutionMode: InProcess\n</code></pre>"},{"location":"02_setup/#8-restart-matlab-if-necessary","title":"8. Restart MATLAB (if necessary)","text":"<ul> <li>check if pyenv is set correctly in MATLAB console   <pre><code>pyenv\n</code></pre></li> </ul>"},{"location":"02_setup/#troubleshooting","title":"Troubleshooting","text":"<p>For any of the installation steps above, you may need to uninstall the previous installation. Sometimes PyTorch cannot make use of CUDA due to incompatibility.</p>"},{"location":"02_setup/#first-option-uninstall","title":"First option - Uninstall","text":"<pre><code>pip uninstall torch torchvision \npip cache purge\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121  \n</code></pre>"},{"location":"02_setup/#second-option-remove-virtual-environment","title":"Second option - Remove Virtual Environment","text":"<ul> <li>Remove the directory recursively   <pre><code>rmdir /s /q .venv\n</code></pre></li> <li>Restart from step Python Step 4 or MATLAB Step 4</li> </ul>"},{"location":"03_python/","title":"Python","text":"<p>In <code>/root</code> of the project are two main script files for 1. training new models and 2. denoising RAW + Test images:</p> <ol> <li><code>training_raw.py</code></li> <li><code>denoise_raw.py</code></li> </ol>"},{"location":"03_python/#1-training_rawpy","title":"1. <code>training_raw.py</code>","text":"<p>Use this script to train new models. When starting the training process there will be a unique model name assigned in the format <code>YYYY_MM_DD_HH_MM_SS</code> and the results will be saved in the directory <code>/model_zoo</code>.</p>"},{"location":"03_python/#setup-training","title":"Setup Training","text":"<p>At the bottom of the script you can set training parameters: <pre><code>if __name__ == '__main__':\n    torch.cuda.empty_cache()\n\n    if 1: # Set to 1 to use pretrained model\n        model_dir = os.path.join('model_zoo', '2024_08_28_10_17_35')\n        pretrained_model_path = util.get_last_saved_model(model_dir)\n    else:\n        pretrained_model_path = None\n\n    opt = {\n        'in_res': 256,\n        'n_channels': 4,\n        'sigma': None,\n        'config': [4] * 7,\n        'batch_size': 24,\n        'n_epochs': 30,\n        'lr_first': 1e-4,\n        'max_train': 1*10**9,\n        'max_test': 250,\n        'pretrained_model_path': pretrained_model_path,\n        'lr_decay': 0.9,\n        'lr_decay_step': 'auto_plateau',\n        'lr_patience': 10,\n        'dpr': 0.0,\n        'use_sigmoid': False,\n        'use_amp': True,\n        'path_train': [\n            os.path.join('training_data', 'LSDIR', 'train'),\n            os.path.join('training_data', 'custom')\n            ],\n        'path_test': [\n            os.path.join('training_data', 'LSDIR','val','HR')\n            ],\n    }\n\n    main(opt)\n</code></pre></p> <p>Info</p> <p>Set <code>use_amp=true</code> and <code>use_sigmoid=false</code> only when using simple loss functions (mae/mse).</p> <p>When using GAN or feature loss set <code>use_amp=false</code> and <code>use_sigmoid=true</code>.</p>"},{"location":"03_python/#setup-loss","title":"Setup Loss","text":"<p>The loss function can be defined inside the CombinedLoss class at this point in the script: <pre><code>criterion = CombinedLoss(\n    pixel_loss_type='L1', pixel_loss_weight=1,\n    gan_loss_type='wgan', gan_loss_weight=1,\n    feature_loss_type='lpips_vgg', feature_loss_weight=1,\n    n_channels=opt['n_channels'])\n</code></pre></p> <p>Info</p> <p>Inspect <code>CombinedLoss.py</code> to see what loss variants are available:</p> <ul> <li>Absolute pixel losses: <code>L1</code>(mae) and <code>L2</code>(mse)</li> <li>GAN losses (realism): <code>gan</code>, <code>wgan</code>, <code>lsgan</code>, <code>ragan</code>, <code>softplusgan</code></li> <li>Feature losses: <code>vgg</code>, <code>lpips_vgg</code>, <code>lpips_alex</code></li> <li>Structural losses: <code>SSIM</code>, <code>MS-SSIM</code> (Not suitable for training)</li> </ul>"},{"location":"03_python/#2-denoise_rawpy","title":"2. <code>denoise_raw.py</code>","text":"<p>Use this script to load RAW images (<code>.dng</code>, <code>.nef</code>, <code>.arw</code>, ...) in RGGB-format. RAW images are loaded via the RawPy and modified in place (add noise / denoise) at the initial processing stage. After denoising the rest of the processing is again entirely handled by RawPy (a LibRaw wrapper for Python).</p> <p>Set the following parameters to your liking:</p> <pre><code>    config = {\n        'raw_dir': os.path.join('test_imgs', 'raw'),\n        'raw_file_index': 5,\n        'model_name': 'l1x10_wgan_lpipsvgg_12500_sigmoid_vst',\n        'iso': 12500,\n        'start_x': 3600,\n        'start_y': 2000,\n        'crop_size': 512,\n        'save_images': True,\n        'test_size': 512,\n        'min_val': 0.2,\n        'max_val': 0.8,\n    }\n</code></pre> <p>The example above</p> <ul> <li>looks for the 5<sup>th</sup> image in the <code>test_imgs/raw</code> directory</li> <li>uses the SCUnet model <code>l1x10_wgan_lpipsvgg_12500_sigmoid_vst</code></li> <li>sets the ISO to 12500</li> <li>crops the image to <code>(512, 512)</code> at <code>(3600, 2000)</code></li> <li>saves the denoising results into the <code>results</code> directory</li> <li>sets the test image size to <code>(512, 512)</code></li> <li>sets the min and max values of the test image to <code>0.2</code> and <code>0.8</code></li> </ul> <p>To generate a synthetic test image (Siemensstar, Gradient Ramps, Zoneplates etc.) as grayscale or colored images you change the following parameters to your liking:</p> <pre><code>TEST_IMAGE_OPTIONS = {\n    'zone_plate': {'max_freq': 32},\n    'harmonic_star': {'num_sectors': 128},\n    'non_harmonic_star': {'num_sectors': 144},\n    'harmonic_star_fixed_contrast': {'num_sectors': 144}, # Contrast level of noise variance at 12500 ISO\n    'non_harmonic_star_fixed_contrast': {'num_sectors': 144},\n    'horizontal_ramp': {'gamma': 2.2},\n    'vertical_ramp': {'gamma': 2.2},\n    'checkerboard': {'squares': 8},\n    'frequency_sweep': {'min_freq': 1, 'max_freq': 32},\n    'circular_zones': {'num_zones': 16},\n    'edge_response': {'edge_width': 15},\n    'colored_zone_plate': {'max_freq': 64},\n    'colored_siemens_star': {'num_sectors': 32},\n    'rgb_ramp': {'direction': 'vertical'},\n    'color_checkerboard': {'squares': 8},\n    'color_frequency_sweep': {'min_freq': 1, 'max_freq': 32},\n    'rainbow_intensity': {},\n    'smpte_color_bars': {},\n    'color_wheel': {}\n}\n</code></pre> <p>When starting the script you will be prompted to enter a number and hit return selecting one of your above defined testimages.</p> <p>All results are put into dynamically created folders inside <code>/results</code> directory in the format of <code>YYYY_MM_DD_HH_MM_SS</code>.</p>"},{"location":"03_python/#have-a-look-at-the-scripts","title":"Have a look at the scripts","text":"TrainingDenoise RAW + Test images training_raw.py<pre><code>import os\nimport time\n\nimport torch\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\n\nfrom utils import utils_image as util\nfrom utils.utils_plot import show_batches, print_batch_minmax\nfrom utils.utils_loss import CombinedLoss\n\n\n\n@torch.no_grad()\ndef validate(model, test_loader, criterion, device, opt):\n    model.eval()\n    total_loss = 0\n    sample_input = None\n    sample_output = None\n    sample_target = None\n    for input_images, target_images in test_loader:\n        input_images = input_images.to(device)\n        target_images = target_images.to(device)\n        if opt['use_amp']:\n            with autocast():\n                output_images = model(input_images)\n                loss = criterion(output_images, target_images)\n        else:\n            output_images = model(input_images)\n            loss = criterion(output_images, target_images)\n        total_loss += loss.item()\n\n        if sample_input is None:\n            sample_input = input_images[:6]\n            sample_output = output_images[:6]\n            sample_target = target_images[:6]\n\n    return total_loss / len(test_loader), sample_input, sample_output, sample_target\n\n\ndef main(opt):\n    # --------------------------------------------\n    # Device and model configuration\n    # --------------------------------------------\n    device = util.get_device()\n    model = util.load_model(device, opt)\n    model_dir = util.make_dir(opt)\n    opt['model_architecture'] = model.__class__.__name__\n    writer = SummaryWriter(os.path.join('tensorboard_logs', os.path.basename(model_dir)))\n    # Enable cuDNN auto-tuner\n    torch.backends.cudnn.benchmark = True\n\n\n    # --------------------------------------------\n    # Data preparation\n    # --------------------------------------------\n    transform_train = transforms.Compose([\n        util.Downsample(),\n        util.RandomCrop(h=opt['in_res'], w=opt['in_res']),\n        util.RgbToRawTransform(noise_model='mixed', iso=12500, wb_gains_mode='normal'),\n        # util.VST(iso=12500),\n        util.ToTensor2()\n    ])\n    transform_test = transforms.Compose([\n        util.Downsample(),\n        util.RandomCrop(h=opt['in_res'], w=opt['in_res']),\n        util.RgbToRawTransform(noise_model='mixed', iso=12500, wb_gains_mode='normal'),\n        # util.VST(iso=12500),\n        util.ToTensor2()\n    ])\n    opt['transforms_train'] = util.extract_transforms_to_dict(transform_test)\n    opt['transforms_test'] = util.extract_transforms_to_dict(transform_test)\n\n\n    train_dataset = util.MyDataset(opt['path_train'], opt['n_channels'], num_images=opt['max_train'], transform=transform_train)\n    test_dataset = util.MyDataset(opt['path_test'], opt['n_channels'], num_images=opt['max_test'], transform=transform_test)\n\n    num_workers = util.get_num_workers()\n    train_loader = DataLoader(dataset=train_dataset, batch_size=opt['batch_size'], shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True, persistent_workers=True)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=opt['batch_size'], shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True, persistent_workers=True)\n    opt['n_img_train'] = train_dataset.__len__()\n    opt['n_img_test'] = test_dataset.__len__()\n    print(f'Training images: {opt[\"n_img_train\"]}, Testing images: {opt[\"n_img_test\"]}')\n\n\n    # --------------------------------------------\n    # Loss &amp; Training params\n    # --------------------------------------------\n    criterion = CombinedLoss(\n        pixel_loss_type='L1', pixel_loss_weight=1,\n        # gan_loss_type='wgan', gan_loss_weight=1,\n        # feature_loss_type='lpips_vgg', feature_loss_weight=1,\n        n_channels=opt['n_channels'])\n\n\n    optimizer = optim.Adam(model.parameters(), lr=opt['lr_first'], eps=1e-7)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=opt['lr_decay'], patience=opt['lr_patience'], min_lr=opt['lr_first'] / 2**5)\n    scaler = GradScaler()\n    opt['loss_info'] = criterion.info\n    opt['optimizer'] = optimizer.__class__.__name__\n    opt['total_iterations'] = len(train_loader) * opt['n_epochs']\n    start_time = time.time()\n\n\n    # --------------------------------------------\n    # Training\n    # --------------------------------------------\n    for epoch in range(opt['n_epochs']):\n        model.train()\n        for i, (input_images, target_images) in enumerate(train_loader):\n            input_images = input_images.to(device)\n            target_images = target_images.to(device)\n            optimizer.zero_grad()\n            if opt['use_amp']:\n                with autocast():\n                    output_images = model(input_images)\n                    if criterion.gan_loss_weight != 0:\n                        criterion.optimize_D(output_images, target_images)\n                    loss = criterion(output_images.float(), target_images.float())\n            else:\n                output_images = model(input_images)\n                if criterion.gan_loss_weight != 0:\n                    criterion.optimize_D(output_images, target_images)\n                loss = criterion(output_images, target_images)\n\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.4)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            opt['iteration'] = epoch * len(train_loader) + i + 1\n            progress = opt['iteration'] / opt['total_iterations'] * 100\n            util.print_progress_info(start_time, opt, progress, criterion, epoch)\n\n            if i % 20 == 0:  # Log every 20 iterations\n                writer.add_scalar('Loss/Train', loss.item(), opt['iteration'])\n\n\n            # --------------------------------------------\n            # Validation, saving model &amp; training info\n            # --------------------------------------------\n            if util.savepoint(opt):\n                val_loss, sample_input, sample_output, sample_target = validate(model, test_loader, criterion, device, opt)\n                writer.add_scalar('Loss/Validation', val_loss, opt['iteration'])\n\n                # Add images to TensorBoard\n                fig = show_batches(sample_input, sample_output, sample_target, return_fig=True)\n                writer.add_figure('Validation Images', fig, opt['iteration'])\n                plt.close(fig)\n\n                # Save model and training info\n                torch.save(model.state_dict(), os.path.join(model_dir, f'model_{opt[\"iteration\"]}.pth'))\n                opt['lr_last'] = scheduler.get_last_lr()[0]\n                util.save_training_info(opt, model_dir, loss.item(), val_loss, model, epoch, time.time() - start_time)\n\n                scheduler.step(val_loss)\n\n    writer.close()\n\n\n\nif __name__ == '__main__':\n    torch.cuda.empty_cache()\n\n    if 1: # Set to 1 to use pretrained model\n        model_dir = os.path.join('model_zoo', '2024_08_28_10_17_35')\n        pretrained_model_path = util.get_last_saved_model(model_dir)\n    else:\n        pretrained_model_path = None\n\n    opt = {\n        'in_res': 256,\n        'n_channels': 4,\n        'sigma': None,\n        'config': [4] * 7,\n        'batch_size': 24,\n        'n_epochs': 30,\n        'lr_first': 1e-4,\n        'max_train': 1*10**9,\n        'max_test': 250,\n        'pretrained_model_path': pretrained_model_path,\n        'lr_decay': 0.9,\n        'lr_decay_step': 'auto_plateau',\n        'lr_patience': 10,\n        'dpr': 0.0,\n        'use_sigmoid': False,\n        'use_amp': True,\n        'path_train': [\n            os.path.join('training_data', 'LSDIR', 'train'),\n            os.path.join('training_data', 'custom')\n            ],\n        'path_test': [\n            os.path.join('training_data', 'LSDIR','val','HR')\n            ],\n    }\n\n    main(opt)\n</code></pre> denoise_raw.py<pre><code>import os\nimport datetime\n\nimport utils.utils_evaluation as u\nimport utils.utils_image as ui\n\n# Dictionary of available test image types and their parameters\nTEST_IMAGE_OPTIONS = {\n    'zone_plate': {'max_freq': 32},\n    'harmonic_star': {'num_sectors': 128},\n    'non_harmonic_star': {'num_sectors': 144},\n    'harmonic_star_fixed_contrast': {'num_sectors': 144},\n    'non_harmonic_star_fixed_contrast': {'num_sectors': 144},\n    'horizontal_ramp': {'gamma': 2.2},\n    'vertical_ramp': {'gamma': 2.2},\n    'checkerboard': {'squares': 8},\n    'frequency_sweep': {'min_freq': 1, 'max_freq': 32},\n    'circular_zones': {'num_zones': 16},\n    'edge_response': {'edge_width': 15},\n    'colored_zone_plate': {'max_freq': 64},\n    'colored_siemens_star': {'num_sectors': 32},\n    'rgb_ramp': {'direction': 'vertical'},\n    'color_checkerboard': {'squares': 8},\n    'color_frequency_sweep': {'min_freq': 1, 'max_freq': 32},\n    'rainbow_intensity': {},\n    'smpte_color_bars': {},\n    'color_wheel': {}\n}\n\n\ndef main():\n    # Configuration dictionary\n    config = {\n        'raw_dir': os.path.join('test_imgs', 'raw'),\n        'raw_file_index': 5,\n        'model_name': 'l1x10_wgan_lpipsvgg_12500_sigmoid_vst',\n        'iso': 12500,\n        'start_x': 3600,\n        'start_y': 2000,\n        'crop_size': 512,\n        'save_images': True,\n        'test_size': 512,\n        'min_val': 0.2,\n        'max_val': 0.8,\n    }\n\n    # Select test image type\n    config['test_image_type'] = u.select_test_image_type(TEST_IMAGE_OPTIONS)\n    config.update(TEST_IMAGE_OPTIONS[config['test_image_type']])\n\n    # Select Raw Image\n    path_files = [f for f in os.listdir(config['raw_dir']) if os.path.isfile(os.path.join(config['raw_dir'], f))]\n    config['raw_path'] = os.path.join(config['raw_dir'], path_files[config['raw_file_index']])\n\n    # Create Result Directory\n    now = datetime.datetime.now()\n    unique_id = now.strftime('%Y_%m_%d_%H_%M_%S')\n    config['result_dir'] = os.path.join('results', unique_id)\n    os.makedirs(config['result_dir'], exist_ok=True)\n\n    # Select SCUNet Model and set VST mode by model name\n    model_path = os.path.join('model_zoo', config['model_name'])\n    last_saved_model_path = ui.get_last_saved_model(model_path)\n    config['scunet_model'] = u.load_model(last_saved_model_path)\n    config['use_vst'] = 'vst' in config['model_name'].lower()\n\n    processor = u.DenoiseProcessor(config)\n\n    # Run Raw Denoising\n    processor.process_raw(config)\n\n    # Run Test Image Denoising\n    config['result_dir'] = config['result_dir'] + '_test'\n    os.makedirs(config['result_dir'], exist_ok=True)\n    processor.result_dir = config['result_dir']\n    processor.process_test(config)\n\n    print('Done!')\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"04_matlab/","title":"MATLAB","text":"<p>Locate the <code>/matlab</code>directory inside <code>/root</code> as your working directory to execute all MATLAB related scripts and functions.</p> <p>Three main scripts:</p> <ol> <li><code>main_compare.m</code></li> <li><code>main_make_results_scunet.m</code></li> <li><code>main_analyze.m</code></li> </ol>"},{"location":"04_matlab/#1-main_comparem","title":"1. <code>main_compare.m</code>","text":"<p>Select two SCUNet models with <code>model_name = &lt;model_name&gt;.pth</code> <pre><code>model_name_1 = 'scunet_l1x10_wgan_lpipsvgg_12500_sigmoid.pth';\nmodel_name_2 = 'scunet_l1x10_wgan_lpipsvgg_12500_sigmoid_vst.pth';\n</code></pre> and compare it automatically with the DnCNN denoiser.</p>"},{"location":"04_matlab/#2-main_make_results_scunetm","title":"2. <code>main_make_results_scunet.m</code>","text":"<p>Again set your desired model name with <code>model_name = &lt;model_name&gt;.pth</code> <pre><code>model_name = 'scunet_l1x10_wgan_lpipsvgg_12500_sigmoid_vst';\n</code></pre> and set the image type (<code>'SiemHarm'</code> or <code>'Verlauf'</code>) by commenting one of these lines: <pre><code>% image_type = \"SiemHarm\";\nimage_type = \"Verlauf\";\n</code></pre></p> <p>The Results will be saved in the <code>/results_dn_bayer</code> directory in the pattern <code>model_name_image_type.mat</code>.</p>"},{"location":"04_matlab/#3-main_analyzem","title":"3. <code>main_analyze.m</code>","text":"<p>This is where you analyze your results from the previous script. Comment the desired line in the script to use <code>'SiemHarm'</code> or <code>'Verlauf'</code>: <pre><code>% image_type = \"SiemHarm\";\nimage_type = \"Verlauf\";\n</code></pre> Based on the chosen image type string, you are prompted to choose a matching result file from the <code>/results_dn_bayer</code> directory. All final results (including, images, heatmaps, plots and metrics) will be saved in the <code>/results_analyze</code> directory.</p> <p>Warning</p> <p>THIS TAKES A WHILE TO RUN</p>"},{"location":"05_venv/","title":"Virtual Environments","text":""},{"location":"05_venv/#introduction","title":"Introduction","text":"<p>When working on multiple Python projects, it is common to encounter conflicts between different dependencies. To manage these potential conflicts and maintain a clean development environment, using a virtual environment (venv) is considered best practice. In this tutorial, we will explore the reasons why utilizing <code>venv</code> is beneficial for your Python projects.</p>"},{"location":"05_venv/#isolation-of-dependencies","title":"Isolation of Dependencies","text":"<p>One of the primary reasons to use a virtual environment is to isolate the dependencies of your projects. Each project can have its own set of libraries and versions, independent of other projects. This isolation helps in avoiding version conflicts and ensures that updating a library for one project does not break another.</p>"},{"location":"05_venv/#easy-management","title":"Easy Management","text":"<p>With a virtual environment, managing project-specific dependencies becomes straightforward. For instance, you can easily track the packages that are installed in each environment using a <code>requirements.txt</code> file. This makes it simpler to recreate or share the environment setup with others:</p> <pre><code>pip freeze &gt; requirements.txt\npip install -r requirements.txt\n</code></pre>"},{"location":"05_venv/#simplified-deployment","title":"Simplified Deployment","text":"<p>When you deploy your project, you can be certain about the environment it was developed and tested in. This reduces the chances of encountering issues due to differing library versions between development and production environments.</p>"},{"location":"05_venv/#environment-consistency","title":"Environment Consistency","text":"<p>Using a virtual environment ensures that all team members are working with the same set of dependencies. This consistency is critical for collaborative projects and can save time troubleshooting bugs that arise from mismatched libraries across different development setups.</p>"},{"location":"05_venv/#avoiding-global-package-installations","title":"Avoiding Global Package Installations","text":"<p>Installing packages globally can lead to a cluttered and sometimes unstable system environment. By using <code>venv</code>, you restrict the package installations to a particular project, preventing potential conflicts with globally installed packages.</p>"},{"location":"05_venv/#setting-up-a-virtual-environment","title":"Setting Up a Virtual Environment","text":"<p>Creating and activating a virtual environment in Python is simple:</p> <p>Here\u2019s the correctly formatted version:</p> <ol> <li> <p>Create the virtual environment:</p> <pre><code>python -m venv myenv\n</code></pre> </li> <li> <p>Activate the virtual environment:</p> <ul> <li> <p>On Windows:</p> <pre><code>myenv\\Scripts\\activate\n</code></pre> </li> <li> <p>On macOS and Linux:</p> <pre><code>source myenv/bin/activate\n</code></pre> </li> </ul> </li> </ol> <p>Once activated, you can install and manage your project's dependencies within this isolated environment. How to set up a venv for the denoising project is featured in the actual python and MATLAB guides on this website.</p>"},{"location":"06_training_tips/","title":"Training Tips","text":""},{"location":"06_training_tips/#optimize-training-performance","title":"Optimize training performance","text":"<ul> <li>DataLoader Setup</li> <li>cuDNN Auto-Tuner</li> <li>Learning Rate Scheduler</li> <li>Mixed Precision Training</li> <li>Gradient Clipping</li> </ul>"},{"location":"06_training_tips/#logging","title":"Logging","text":"<ul> <li>TensorBoard</li> <li>Checkpointing</li> </ul>"},{"location":"06_training_tips/#dataloader-setup","title":"DataLoader Setup","text":"<p><code>num_workers</code></p> <ul> <li> <p>Allows data to be loaded/preprocessed in parallel, ensuring the GPU is continuously fed with new batches.</p> </li> <li> <p>Rule of thumb: <code>num_workers = num_cpu_cores</code> (can be higher or lower depending on your system).</p> </li> </ul> <p><code>shuffle=True</code></p> <ul> <li>Shuffles the order of the batches before each epoch to prevent patterns from forming.</li> </ul> <p><code>pin_memory=True</code></p> <ul> <li>Pre-allocates memory for faster data transfer.</li> </ul> <p><code>drop_last=True</code></p> <ul> <li>Ensures consistent batch size for uniform computation graphs.</li> </ul> <p><code>persistent_workers=True</code></p> <ul> <li>Keeps workers active across epochs instead of reinitializing them every time.</li> </ul>"},{"location":"06_training_tips/#cudnn-auto-tuner","title":"cuDNN Auto-Tuner","text":"<p><code>torch.backends.cudnn.benchmark = True</code></p> <ul> <li>The auto-tuner tests different implementations of algorithms (e.g., for convolution, pooling) and selects the fastest one that can run on the current hardware.</li> <li>Note: The input size should be constant (which is usually the case). If the input size is dynamic, a recalculation might be triggered, leading to potential inefficiencies.</li> </ul>"},{"location":"06_training_tips/#learning-rate-scheduler-reducelronplateau","title":"Learning Rate Scheduler: ReduceLROnPlateau","text":"<p><code>ReduceLROnPlateau(mode='min', factor, patience, min_lr)</code></p> <ul> <li>Dynamic Learning Rate Adjustment: The learning rate is dynamically adjusted during training.</li> <li>Mechanism: If the loss stagnates or does not decrease further for a number of validation iterations specified by <code>patience</code>, the learning rate is multiplied by <code>factor</code> until the optional minimum <code>min_lr</code> is reached.</li> <li>Recommendation: Use moderate values like <code>factor=0.9</code> to avoid \"stalling\" in the training process.</li> </ul>"},{"location":"06_training_tips/#amp-automatic-mixed-precision","title":"AMP: Automatic Mixed Precision","text":"<p>Running the Model with <code>autocast()</code> and Adjusting Loss with <code>GradScaler()</code>.</p> <ul> <li>Mixed Precision Arithmetic: Combines 16-bit (half-precision) and 32-bit (single-precision) floating-point arithmetic.</li> <li>Efficiency Improvement: Enhances training efficiency without compromising model accuracy.</li> <li>Increased Batch Size: Allows for nearly double the batch size since only critical operations like summations are performed with 32-bit precision. Note: </li> <li>Works effectively with \"simple\" loss functions (e.g., MAE, MSE).</li> <li>Not compatible with GAN and feature-based loss functions.</li> </ul>"},{"location":"06_training_tips/#gradient-clipping","title":"Gradient Clipping","text":"<pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.4)\n</code></pre> <ul> <li>Preventing Large Gradients: Ensures that gradients do not become too large, which could destabilize the training process.</li> <li>Reducing the Risk of Exploding Gradients: Minimizes the risk of gradients \"exploding\" during backpropagation.</li> <li>Controlled Model Updates: Updates to the model parameters are scaled, while maintaining the direction of the gradient vector.</li> </ul> <p>Motivation:  - The GAN loss often collapsed to NaN after extended training. - The value of <code>max_norm=0.4</code> was empirically determined to address this issue.</p>"},{"location":"06_training_tips/#logging-with-tensorboard","title":"Logging with TensorBoard","text":""},{"location":"06_training_tips/#usage-with-summarywriter","title":"Usage with SummaryWriter:","text":"<ul> <li>Purpose: Visualization of training and validation losses, as well as output images.</li> </ul>"},{"location":"06_training_tips/#opening-tensorboard-in-vs-code","title":"Opening TensorBoard in VS Code:","text":"<ol> <li>Open the Command Palette:<ul> <li>Shortcut: <code>Ctrl+Shift+P</code></li> </ul> </li> <li>Search for TensorBoard:<ul> <li>Type: <code>\"Python: Launch TensorBoard\"</code></li> <li>Press <code>Enter</code></li> </ul> </li> <li>Select the Logs Folder:<ul> <li>Choose: <code>Select another folder</code></li> <li>Navigate to: <code>tensorboard_logs</code></li> </ul> </li> <li>View in Browser:<ul> <li>Open: <code>http://localhost:6006/</code></li> </ul> </li> </ol>"},{"location":"06_training_tips/#logging-training-info-with-json","title":"Logging training info with <code>.json</code>","text":"<p>Automatic info inside the <code>/model_zoo</code> next to each trained model which looks something like this: <pre><code>{\n    \"in_res\": 256,\n    \"n_channels\": 4,\n    \"epoch\": 30,\n    \"total_epochs\": 30,\n    \"iteration\": 224970,\n    \"total_iterations\": 224970,\n    \"duration\": 43.3434,\n    \"pretrained_model\": \"model_zoo\\\\2024_08_13_08_15_32\\\\model_96750.pth\",\n    \"model_architecture\": \"SCUNet\",\n    \"use_sigmoid\": true,\n    \"config\": \"[4,4,4,4,4,4,4]\",\n    \"batch_size\": 12,\n    \"loss_info\": {\n        \"Pixel_Loss\": \"L1\",\n        \"Pixel_Loss_Weight\": 10,\n        \"GAN_Loss\": \"wgan\",\n        \"GAN_Loss_Weight\": 1,\n        \"Feature_Loss\": \"lpips_vgg\",\n        \"Feature_Loss_Weight\": 1\n    },\n    \"optimizer\": \"Adam\",\n    \"initial_learning_rate\": 0.0001,\n    \"last_learning_rate\": 5.904900000000002e-05,\n    \"decay_factor\": 0.9,\n    \"decay_step\": \"auto_plateau\",\n    \"lr_patience\": 10,\n    \"number_params\": 17947224,\n    \"n_imgs_train\": 89991,\n    \"n_imgs_test\": 250,\n    \"training_path\": [\n        \"training_data\\\\LSDIR\\\\train\",\n        \"training_data\\\\custom\"\n    ],\n    \"testing_path\": [\n        \"training_data\\\\LSDIR\\\\val\\\\HR\"\n    ],\n    \"transforms_train\": {\n        \"0\": {\n            \"class_name\": \"Downsample\",\n            \"downsampling_factor\": 2,\n            \"sigma\": 1\n        },\n        \"1\": {\n            \"class_name\": \"RandomCrop\",\n            \"h\": 256,\n            \"w\": 256\n        },\n        \"2\": {\n            \"class_name\": \"RgbToRawTransform\",\n            \"iso\": 12500,\n            \"noise_model\": \"dng\",\n            \"wb_gains_mode\": \"normal\"\n        },\n        \"3\": {\n            \"class_name\": \"ToTensor2\"\n        }\n    },\n    \"transforms_test\": {\n        \"0\": {\n            \"class_name\": \"Downsample\",\n            \"downsampling_factor\": 2,\n            \"sigma\": 1\n        },\n        \"1\": {\n            \"class_name\": \"RandomCrop\",\n            \"h\": 256,\n            \"w\": 256\n        },\n        \"2\": {\n            \"class_name\": \"RgbToRawTransform\",\n            \"iso\": 12500,\n            \"noise_model\": \"dng\",\n            \"wb_gains_mode\": \"normal\"\n        },\n        \"3\": {\n            \"class_name\": \"ToTensor2\"\n        }\n    },\n    \"final_test_loss\": 0.054455384612083435,\n    \"final_val_loss\": 0.028505839593708514\n}\n</code></pre></p>"},{"location":"06_training_tips/#bonus","title":"Bonus","text":"<p>If you ever need to prevent your computer from shutting down, use this batch script: <pre><code>@echo off\nsetlocal enabledelayedexpansion\n\nrem Ermitteln der aktuellen Stunde\nset \"currentHour=%TIME:~0,2%\"\nset \"currentHour=!currentHour: =!\" rem Leerzeichen entfernen\n\nrem Berechnen der Start- und Endzeit f\u00fcr die n\u00e4chsten 18 Stunden\nset /a \"startHour=!currentHour!\"\nset /a \"endHour=(startHour + 18) %% 24\"\n\nrem Aktualisieren der \"Active Hours\" im Registrierungseditor\nreg add \"HKLM\\SOFTWARE\\Microsoft\\WindowsUpdate\\UX\\Settings\" /v \"ActiveHoursStart\" /t REG_DWORD /d !startHour! /f\nreg add \"HKLM\\SOFTWARE\\Microsoft\\WindowsUpdate\\UX\\Settings\" /v \"ActiveHoursEnd\" /t REG_DWORD /d !endHour! /f\n\necho Die \"Active Hours\" wurden auf !startHour! bis !endHour! Uhr festgelegt.\n</code></pre></p>"},{"location":"07_models/","title":"Trained Models","text":""},{"location":"07_models/#l1_12500","title":"l1_12500","text":"<ul> <li>Description: Absolute Error (L1 / MAE)</li> <li>Noise: ISO Noise 12500</li> </ul>"},{"location":"07_models/#l1_12500_vst","title":"l1_12500_vst","text":"<ul> <li>Description: Absolute Error (L1 / MAE)</li> <li>Noise: ISO Noise 12500</li> <li>Additional Processing: Variance Stabilizing Transform (VST)</li> </ul>"},{"location":"07_models/#l1x10_wgan_lpipsvgg_12500_sigmoid","title":"l1x10_wgan_lpipsvgg_12500_sigmoid","text":"<ul> <li>Description: Absolute Error (L1 / MAE) x 10, GAN-Loss (WGAN), Feature-Loss (LPIPS-VGG)</li> <li>Noise: ISO Noise 12500</li> <li>Final Layer: Sigmoid</li> </ul>"},{"location":"07_models/#l1x10_wgan_lpipsvgg_12500_sigmoid_vst","title":"l1x10_wgan_lpipsvgg_12500_sigmoid_vst","text":"<ul> <li>Description: Absolute Error (L1 / MAE) x 10, GAN-Loss (WGAN), Feature-Loss (LPIPS-VGG)</li> <li>Noise: ISO Noise 12500</li> <li>Final Layer: Sigmoid</li> <li>Additional Processing: Variance Stabilizing Transform (VST)</li> </ul>"},{"location":"07_models/#l1_semiblind_vst-in-progress","title":"l1_semiblind_vst (in progress)","text":"<ul> <li>Description: Absolute Error (L1 / MAE)</li> <li>Noise Factors: Variable Noise Factor (NF) = [1, 1.5] </li> <li>ISO Noise 12500 * NF</li> <li>AWGN * NF</li> <li>ISO + AWGN * NF</li> <li>Additional Processing: Variance Stabilizing Transform (VST)</li> </ul>"}]}